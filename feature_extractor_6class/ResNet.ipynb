{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTApeE4tFuJO"
   },
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h3Y-OP0ZJFke"
   },
   "outputs": [],
   "source": [
    "# device = 'cpu'\n",
    "map_vpc = {'1': 0, '3': 1, '4': 2, '5': 3, '0': 4, '6': 5} # map labels: 1 = benign (0), we don't have 2, i = Gleason i (i-2) for i=[3:5]\n",
    "num_classes = 6\n",
    "batch_size = 32\n",
    "magnification=40\n",
    "stains = ['HnE']\n",
    "fold = 'fold1'\n",
    "\n",
    "if fold == 'fold1':\n",
    "    train_slides_vpc = [2, 5, 6, 7]\n",
    "    val_slides_vpc = [3]\n",
    "    test_slides_vpc = [1]\n",
    "elif fold == 'fold2':\n",
    "    train_slides_vpc = [1, 3, 6, 7]\n",
    "    val_slides_vpc = [2]\n",
    "    test_slides_vpc = [5]\n",
    "elif fold == 'fold3':\n",
    "    train_slides_vpc = [1, 2, 3, 5]\n",
    "    val_slides_vpc = [7]\n",
    "    test_slides_vpc = [6]\n",
    "# else:\n",
    "#     assert False 'Please choose the correct fold! - {\"fold1\", \"fold2\", \"fold3\"}'\n",
    "\n",
    "# model_results_path = 'model/' + str(magnification) + '/' + fold + '/aug_model'\n",
    "model_results_path = 'model_VPC_Zurich_Colorado/' + str(magnification) + '/' + fold + '/256_aug_model'\n",
    "\n",
    "\n",
    "# path_VPC = '../data/DeepPath_BlockImages/'\n",
    "path_VPC = 'VPC_staintools/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUEUvDLIbLnw"
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qQb9tJzebNW_"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from skimage import io\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import staintools\n",
    "import random\n",
    "import torchvision.transforms.functional as TF\n",
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sH-6cE6oHyQD"
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h-w_nD4xGfGB"
   },
   "outputs": [],
   "source": [
    "# a function to move tensors from the CPU to the GPU\n",
    "def dict_to_device(orig, device):\n",
    "    new = {}\n",
    "    for k,v in orig.items():\n",
    "        new[k] = v.to(device)\n",
    "    return new\n",
    "\n",
    "def plotImage(img, ax=plt):\n",
    "    img_pil = torchvision.transforms.ToPILImage()(img)\n",
    "    img_size = torch.FloatTensor(img_pil.size)\n",
    "    ax.imshow(img_pil)\n",
    "    \n",
    "class MyRotationTransform:\n",
    "    \"\"\"Rotate by one of the given angles.\"\"\"\n",
    "\n",
    "    def __init__(self, angles):\n",
    "        self.angles = angles\n",
    "\n",
    "    def __call__(self, x):\n",
    "        angle = random.choice(self.angles)\n",
    "        return TF.rotate(x, angle)\n",
    "\n",
    "rotation_transform = MyRotationTransform(angles=[0, 90, 180, 270])\n",
    "    \n",
    "AUGMENTED_TRANSFORM = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    rotation_transform,\n",
    "#     transforms.GaussianBlur(20, sigma=(0,0.1)),\n",
    "    transforms.ColorJitter(0.1, 0.1, 0.1, 0.1)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_CRmJntGgwi"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1LzS0KGzGiqg"
   },
   "outputs": [],
   "source": [
    "class VPCDataset(Dataset): # VPC might be not robust to augmentaion\n",
    "    def __init__(self, root_dir, slides, map1, magnifications, stains, augmentation=False, transform=transforms.ToTensor()):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.map = map1\n",
    "        self.y = []\n",
    "        self.img_files = []\n",
    "        self.ratio = np.zeros(num_classes)\n",
    "        self.augmentation = augmentation\n",
    "        stain_map = {'HnE': 'stain001', 'Ki67': 'stain002', 'P63': 'stain003'}\n",
    "        magnification_map = {10: 'scale001', 20: 'scale002', 40: 'scale003'}\n",
    "        self.stain_name = []\n",
    "        self.magnification_name = []\n",
    "        for stain in stains:\n",
    "            self.stain_name.append(stain_map[stain])\n",
    "        for magnification in magnifications:\n",
    "            self.magnification_name.append(magnification_map[magnification])\n",
    "            \n",
    "        \n",
    "        # extracting image locations\n",
    "        for slide in slides:\n",
    "            slide_path = root_dir + 'slide00' + str(slide) + '/'\n",
    "            if augmentation:\n",
    "                self.img_files.extend([img_file for img_file in os.listdir(slide_path) \n",
    "                                       if (img_file.split('_')[4] in self.magnification_name and\n",
    "                                           img_file.split('_')[3] in self.stain_name)])\n",
    "            else:\n",
    "                self.img_files.extend([img_file for img_file in os.listdir(slide_path) \n",
    "                                       if (img_file.split('_')[4] in self.magnification_name and\n",
    "                                           img_file.split('_')[3] in self.stain_name) and len(img_file.split('_')) == 6])\n",
    "\n",
    "        # get ratio of the class\n",
    "        for img_file in self.img_files:\n",
    "            label = int(self.map[img_file[49]])\n",
    "            self.ratio[label] += 1\n",
    "        self.ratio = 1 / (self.ratio + 1) ## avoid divided by zero \n",
    "        self.ratio /= np.sum(self.ratio)\n",
    "        \n",
    "        if self.augmentation:\n",
    "            self.img_files.sort()\n",
    "\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.augmentation:\n",
    "            return len(self.img_files) // 15\n",
    "        return len(self.img_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        y = None\n",
    "        img = None\n",
    "        \n",
    "        index = idx # if there is no augmentation\n",
    "        if self.augmentation:\n",
    "            index = idx * 15 + random.randint(0, 14) # choose one of the 14 augmentations or image itself, randomly\n",
    "        img_file = self.img_files[index]\n",
    "        img_path = self.root_dir + img_file[:8] + '/' + img_file # hard coded!!!\n",
    "        img = io.imread(img_path)\n",
    "        img = cv.resize(img, (256, 256), interpolation=cv.INTER_CUBIC)\n",
    "        y = int(self.map[img_file[49]])\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            return {'img': img, 'label': y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FT6ZNli6Vviu"
   },
   "outputs": [],
   "source": [
    "dataset_train = VPCDataset(path_VPC, train_slides_vpc, map_vpc, [magnification], stains, True, transform=AUGMENTED_TRANSFORM)\n",
    "dataset_val = VPCDataset(path_VPC, val_slides_vpc, map_vpc, [magnification], stains, True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vGmnLi5lWNUy"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=dataset_train, batch_size=batch_size, num_workers=2, shuffle=True, pin_memory=False)\n",
    "val_loader = DataLoader(dataset=dataset_val, batch_size=batch_size, num_workers=2, shuffle=False, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iJ_JUwOrXAEn"
   },
   "outputs": [],
   "source": [
    "iterator = iter(train_loader)\n",
    "batch = next(iterator)\n",
    "# output = dataset.__getitem__(50)['img']\n",
    "output = batch['img'][1]\n",
    "img = batch['img']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8mdAhfzEYt8L",
    "outputId": "b6ea01dc-c777-40f0-9886-e4818ddc0d1d"
   },
   "outputs": [],
   "source": [
    "torch.min(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "OIiiRxO_Y_jD",
    "outputId": "8cd1c3c0-1d3b-4186-c6a5-664f7ef28636"
   },
   "outputs": [],
   "source": [
    "plt.imshow(output.cpu().permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset_train.img_files))\n",
    "print(len(dataset_train.img_files) / 15)\n",
    "print(len(dataset_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset_val.img_files))\n",
    "print(len(dataset_val.img_files) / 15)\n",
    "print(len(dataset_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8WK9_pRKuNg"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "b5b4b114bade4395952a717e239c6fcb",
      "5a1bbf7b9c904cb295cb486bddacd2f0",
      "40b3dbde3e894eb78f45e9d2342aa714",
      "d62ae91d208241eeb980c7b683cdff1b",
      "2969031089f34cbdbe70c405167673c2",
      "8b13f258874e4d89bdf5640e70cdd3a4",
      "ee13ea848e1c48f4958890da00770737",
      "cc24c5ac94e24a8b8d0e10d07142bf48",
      "e26289495af6433dbf3d4c3f7c134945",
      "ecd2309de7dd41d290f9ed71e085e4d4",
      "0b4c29892f0c4c0e9b7176b867aca14c"
     ]
    },
    "id": "5BE0iu3cRmFg",
    "outputId": "e3877b44-f824-4161-f869-72c51e82b1e0"
   },
   "outputs": [],
   "source": [
    "class NN(torch.nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "#         self.num_classes = num_classes\n",
    "        self.model = torchvision.models.resnet18(pretrained=True)\n",
    "        self.model.fc = nn.Sequential(nn.Linear(in_features=512, out_features=num_classes, bias=True, ))#,\n",
    "                         # nn.ReLU(),\n",
    "                         # nn.Linear(in_features=128, out_features=32, bias=True),\n",
    "                         # nn.ReLU(),\n",
    "                         # nn.Dropout(0.2),\n",
    "                         # nn.Linear(in_features=32, out_features=num_classes, bias=True))\n",
    "#         self.model = torchvision.models.resnet50(pretrained=True)\n",
    "#         self.model.fc.out_features = num_classes\n",
    "#         self.model.fc = nn.Sequential(nn.Linear(in_features=2048, out_features=num_classes, bias=True, ))#,\n",
    "                        #  nn.ReLU(),\n",
    "                        #  nn.Linear(in_features=1000, out_features=num_classes, bias=True))\n",
    "        print(self.model)\n",
    "\n",
    "    def forward(self, dictionary):\n",
    "        return {'label': self.model(dictionary['img'])}\n",
    "\n",
    "    def prediction(self, dictionary):\n",
    "        return {'label': torch.argmax(self.forward(dictionary)['label'], dim=1)}\n",
    "\n",
    "model = NN(num_classes=num_classes).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QuZbOd0mMwQn"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSkPm1xQWeL8"
   },
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "UppARf88Y8Xe",
    "outputId": "eb3102a5-75cf-4426-8723-c84a2561351f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prepare plotting\n",
    "fig = plt.figure(figsize=(20, 5), dpi= 80, facecolor='w', edgecolor='k')\n",
    "axes = fig.subplots(1,3)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[5,15,25], gamma=0.1)\n",
    "\n",
    "sm = nn.Softmax(dim=1)\n",
    "\n",
    "weights = torch.FloatTensor(train_loader.dataset.ratio).to('cuda') # balancing data in loss\n",
    "\n",
    "num_epochs = 41\n",
    "losses = []\n",
    "# val_losses = []\n",
    "val_accs = []\n",
    "val_acc = 0\n",
    "for epoch in range(num_epochs):\n",
    "    train_iter = iter(train_loader)\n",
    "    model.train()\n",
    "    for i in range(len(train_loader)):\n",
    "        batch_cpu = next(train_iter)\n",
    "        batch_gpu = dict_to_device(batch_cpu, 'cuda')\n",
    "        pred = model(batch_gpu)\n",
    "        pred_cpu = dict_to_device(pred, 'cpu')\n",
    "        \n",
    "        ### Change weight to pos_weight, is input and target positions correct?\n",
    "        loss = torch.nn.functional.cross_entropy(pred['label'], nn.functional.one_hot(batch_gpu['label'], num_classes=num_classes).double(), weight = weights)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        \n",
    "        ## plotting ##\n",
    "        if i%20==0:\n",
    "            # clear figures for a new update\n",
    "            for ax in axes:\n",
    "                ax.cla()\n",
    "            # plot the predicted pose and ground truth pose on the image\n",
    "            plotImage(batch_cpu['img'][0], ax=axes[0])\n",
    "            \n",
    "            ## !!!! code below is dependent on the prediction code. The commented code below is a better choice!\n",
    "            axes[0].set_title('Input image with ground truth label = {}, and predicted = {}'.format(batch_cpu['label'][0], int(torch.argmax(pred_cpu['label'][0]))))\n",
    "            # axes[0].set_title('Input image with ground truth label = {}, and predicted = {}'.format(batch_cpu['label'][0], int(model.prediction(batch_gpu)['label'][0])))\n",
    "\n",
    "            # plot the training error on a log plot\n",
    "            axes[1].plot(losses, label='loss')\n",
    "            axes[1].set_yscale('log')\n",
    "            axes[1].set_title('Training loss')\n",
    "            axes[1].set_xlabel('number of gradient iterations')\n",
    "            axes[1].legend()\n",
    "\n",
    "            # plot the training error on a log plot\n",
    "            # axes[2].plot(val_losses, label='val_loss')\n",
    "            axes[2].plot(val_accs, label='val_acc')\n",
    "            # axes[2].set_yscale('log')\n",
    "            axes[2].set_title('Validation Accuracy')\n",
    "            axes[2].set_xlabel('number of epochs')\n",
    "            axes[2].legend()\n",
    "\n",
    "            # clear output window and diplay updated figure\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(plt.gcf())\n",
    "            print(\"Epoch {}, iteration {} of {} ({} %), loss={}\\nval_acc = {}\".format(epoch, i, len(train_loader), 100*i//len(train_loader), losses[-1], val_accs))\n",
    "            # print(\"Training for the specified amount of epochs would take long.\\nStop the process once you verified that the training works on your setup.\")\n",
    "\n",
    "    ## Validation\n",
    "    val_iter = iter(val_loader)\n",
    "    model.eval()\n",
    "    # val_loss = 0\n",
    "    val_acc = 0\n",
    "    pred_val = np.zeros(len(dataset_val))\n",
    "    label = np.zeros(len(dataset_val))\n",
    "    for i in range(len(val_loader)):\n",
    "        batch_cpu = next(val_iter)\n",
    "        batch_gpu = dict_to_device(batch_cpu, 'cuda')\n",
    "        # pred = model(batch_gpu)\n",
    "        # pred_cpu = dict_to_device(pred, 'cpu')\n",
    "        # MSE loss\n",
    "        # val_loss += (torch.nn.functional.cross_entropy(pred['label'], nn.functional.one_hot(batch_gpu['label'], num_classes=num_classes).double()) * batch_gpu['label'].shape[0] / batch_size).item()\n",
    "        ## Accuracy\n",
    "#         val_acc += (torch.sum(model.prediction(batch_gpu)['label'] == batch_gpu['label'])).item()\n",
    "        ## AUC\n",
    "#         if pred_val is None:\n",
    "#             pred_val = sm(model(batch_gpu)['label'].cpu()).detach().numpy()\n",
    "#             label = batch_cpu['label'].numpy()\n",
    "#         else:\n",
    "#             pred_val = np.append(pred_val, sm(model(batch_gpu)['label'].cpu()).detach().numpy(), axis=0)\n",
    "#             label = np.append(label, batch_cpu['label'].numpy())\n",
    "        ## Accuracy\n",
    "        # if pred_val is None:\n",
    "        #     pred_val = model.prediction(batch_gpu)['label'].cpu().detach().numpy()\n",
    "        #     label = batch_cpu['label'].numpy()\n",
    "        # else:\n",
    "        #     pred_val = np.append(pred_val, model.prediction(batch_gpu)['label'].cpu().detach().numpy())\n",
    "        #     label = np.append(label, batch_cpu['label'].numpy())\n",
    "        \n",
    "        pred_val[i*batch_size:i*batch_size + batch_cpu['label'].shape[0]] = model.prediction(batch_gpu)['label'].cpu().detach().numpy()\n",
    "        label[i*batch_size:i*batch_size + batch_cpu['label'].shape[0]] = batch_cpu['label'].numpy()\n",
    "            \n",
    "            \n",
    "    ## AUC\n",
    "#     val_acc = roc_auc_score(label,pred_val,multi_class='ovr')\n",
    "#     val_accs.append(val_acc)\n",
    "\n",
    "    ## Accuracy\n",
    "    val_acc = np.mean(label == pred_val)\n",
    "    val_accs.append(val_acc)\n",
    "\n",
    "    ## saving the model\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "        'val_AUC': val_acc\n",
    "    }, model_results_path + '_' + str(epoch) + '_' + str(val_acc))\n",
    "\n",
    "    # clear output window and diplay updated figure\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    print(\"Epoch {}, iteration {} of {} ({} %), loss={}\\nval_acc = {}\".format(epoch, i, len(train_loader), 100*i//len(train_loader), losses[-1], val_accs))\n",
    "\n",
    "    ### Scheduler ###\n",
    "    scheduler.step()\n",
    "    \n",
    "plt.close('all')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ResNet_baseline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0b4c29892f0c4c0e9b7176b867aca14c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2969031089f34cbdbe70c405167673c2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "40b3dbde3e894eb78f45e9d2342aa714": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cc24c5ac94e24a8b8d0e10d07142bf48",
      "max": 46830571,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e26289495af6433dbf3d4c3f7c134945",
      "value": 46830571
     }
    },
    "5a1bbf7b9c904cb295cb486bddacd2f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8b13f258874e4d89bdf5640e70cdd3a4",
      "placeholder": "​",
      "style": "IPY_MODEL_ee13ea848e1c48f4958890da00770737",
      "value": "100%"
     }
    },
    "8b13f258874e4d89bdf5640e70cdd3a4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b5b4b114bade4395952a717e239c6fcb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5a1bbf7b9c904cb295cb486bddacd2f0",
       "IPY_MODEL_40b3dbde3e894eb78f45e9d2342aa714",
       "IPY_MODEL_d62ae91d208241eeb980c7b683cdff1b"
      ],
      "layout": "IPY_MODEL_2969031089f34cbdbe70c405167673c2"
     }
    },
    "cc24c5ac94e24a8b8d0e10d07142bf48": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d62ae91d208241eeb980c7b683cdff1b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ecd2309de7dd41d290f9ed71e085e4d4",
      "placeholder": "​",
      "style": "IPY_MODEL_0b4c29892f0c4c0e9b7176b867aca14c",
      "value": " 44.7M/44.7M [00:00&lt;00:00, 180MB/s]"
     }
    },
    "e26289495af6433dbf3d4c3f7c134945": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ecd2309de7dd41d290f9ed71e085e4d4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ee13ea848e1c48f4958890da00770737": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
