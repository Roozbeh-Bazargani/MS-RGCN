{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "868c0521",
   "metadata": {},
   "source": [
    "# Parameters\n",
    "Edge_types mapping: 10_10: 0, 20_20: 1, 40_40: 2, 10_20: 3, 20_40: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5fec9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "graph_type = 'heterogeneous' # ['homogeneous', 'heterogeneous']\n",
    "fold = 'fold1' # ['fold1', 'fold2', 'fold3']\n",
    "\n",
    "if fold == 'fold1':\n",
    "    train_slides_vpc = [2, 5, 6, 7]\n",
    "    val_slides_vpc = [3]\n",
    "    test_slides_vpc = [1]\n",
    "elif fold == 'fold2':\n",
    "    train_slides_vpc = [1, 3, 6, 7]\n",
    "    val_slides_vpc = [2]\n",
    "    test_slides_vpc = [5]\n",
    "elif fold == 'fold3':\n",
    "    train_slides_vpc = [1, 2, 3, 5]\n",
    "    val_slides_vpc = [7]\n",
    "    test_slides_vpc = [6]\n",
    "# else:\n",
    "#     assert False 'Please choose the correct fold! - {\"fold1\", \"fold2\", \"fold3\"}'\n",
    "\n",
    "model_path = 'model/{}/model_mag_multi'.format(fold)\n",
    "# path_outcomes = '../data/VPC-TMA/vpc_cores_ladan.csv'\n",
    "# path_outcomes = '../data/Zurich_TMA/Zurich_GG.csv'\n",
    "# path_outcomes = '../data/Colorado/Colorado_GG.csv'\n",
    "path_outcomes = '../data/PANDA/train.csv'\n",
    "\n",
    "# path_VPC = '../feature_extractor_6class/VPC_embeddings/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f70611b",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f23dc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import dgl\n",
    "# from dgl.data import DGLDataset\n",
    "from torch_geometric.data import Dataset\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_networkx, to_undirected\n",
    "import torch\n",
    "import os\n",
    "import networkx as nx # graph visualization\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da350e0",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f987d5f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a function to move tensors from the CPU to the GPU\n",
    "def dict_to_device(orig, device):\n",
    "    new = {}\n",
    "    for k,v in orig.items():\n",
    "        new[k] = v.to(device)\n",
    "    return new\n",
    "\n",
    "def get_index_by_name(name):\n",
    "    temp = name.split('_')\n",
    "    slide = int(temp[0][-3:])\n",
    "    core = int(temp[1][-3:])\n",
    "    return 160*(slide-1) + core - 1\n",
    "\n",
    "def GG_from_core_index(df, name):\n",
    "    index_core = get_index_by_name(name)\n",
    "    label = df.GG_TMA[index_core]\n",
    "    if label == 'B':\n",
    "        label = 0\n",
    "    elif label.isnumeric():\n",
    "        label = int(label)\n",
    "    else:\n",
    "        return -1\n",
    "    return label\n",
    "\n",
    "def is_neighbors(key, key_n, patch_size):\n",
    "    x, y = key.split('_')\n",
    "    x, y = int(x), int(y)\n",
    "    x_n, y_n = key_n.split('_')\n",
    "    x_n, y_n = int(x_n), int(y_n)\n",
    "    return (x_n - x)**2 + (y_n - y)**2 <= patch_size**2\n",
    "\n",
    "## https://stackoverflow.com/questions/5967500/how-to-correctly-sort-a-string-with-a-number-inside\n",
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "def natural_keys(text):\n",
    "    '''\n",
    "    alist.sort(key=natural_keys) sorts in human order\n",
    "    http://nedbatchelder.com/blog/200712/human_sorting.html\n",
    "    (See Toothy's implementation in the comments)\n",
    "    '''\n",
    "    return [ atoi(c) for c in re.split(r'(\\d+)', text) ]\n",
    "\n",
    "def Biomax_GG_from_core(df1, df3, name):\n",
    "    temp = name.split('_')\n",
    "    slide = int(temp[0][-3:])\n",
    "    core = int(temp[1][-3:])\n",
    "    if slide == 1:\n",
    "        GS = df1.gleason_score[core-1]\n",
    "    else:\n",
    "        GS = df3.gleason_score[core-1]\n",
    "    if GS == '-':\n",
    "        return 0\n",
    "    else:\n",
    "        major, minor = int(GS[0]), int(GS[-1])\n",
    "        GS = major + minor\n",
    "        if 2 in (major, minor): # benign in majors\n",
    "            GG = -1\n",
    "        else:\n",
    "            if GS > 8:\n",
    "                GG = 5\n",
    "            elif GS == 8:\n",
    "                GG = 4\n",
    "            elif GS == 6:\n",
    "                GG = 1\n",
    "            else: # GS == 7\n",
    "                if major == 4:\n",
    "                    GG = 3\n",
    "                else: # major = 3\n",
    "                    GG = 2\n",
    "    return GG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a91090",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n1 = np.repeat(np.array([0,1,2,3,4,5,6]),5)\n",
    "e = to_undirected(torch.tensor([[0,0,0,0,0,0],[1,2,3,4,5,6]], dtype=torch.long))\n",
    "edge_index = e.detach()\n",
    "print(edge_index.t())\n",
    "edge_attr = torch.tensor(np.random.rand(35,1))\n",
    "\n",
    "x = torch.tensor([[0], [0], [0], [0], [0], [1], [1]], dtype=torch.float)\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index, edge_attr = edge_attr)\n",
    "print(data)\n",
    "# Data(edge_attr=[35, 1], edge_index=[2, 35], x=[7, 1])\n",
    "\n",
    "networkX_graph = to_networkx(data, node_attrs=[\"x\"], edge_attrs=[\"edge_attr\"])\n",
    "nx.draw_networkx(networkX_graph)\n",
    "\n",
    "### DGL ###\n",
    "# g = dgl.graph(([0,0,0,0,0],[1,2,3,4,5]))\n",
    "# print(g.edges())\n",
    "# gx = dgl.to_networkx(g)\n",
    "# nx.draw_networkx(gx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba726f0",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a46dd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VPCDataset(Dataset):\n",
    "    def __init__(self, root, fold, magnifications, path_outcomes=None, transform=None, pre_transform=None, pre_filter=None):\n",
    "        self.fold = fold\n",
    "        self.magnifications = magnifications\n",
    "        if path_outcomes is None: # Biomax dataset\n",
    "            self.df1 = pd.read_csv('../data/Biomax_TMA/PR1921a.csv').reset_index(drop = True)\n",
    "            self.df3 = pd.read_csv('../data/Biomax_TMA/PR807b.csv').reset_index(drop = True)\n",
    "        else: # Zurich or VPC data\n",
    "            self.df = pd.read_csv(path_outcomes).reset_index(drop = True)\n",
    "#         self.df.columns = [c.replace(' ', '_').replace('/','_') for c in df.columns]\n",
    "        super().__init__(root + fold + '/', transform, pre_transform, pre_filter)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return '../../../../feature_extractor_6class_PANDA/256_Radboud_embeddings/{}'.format(self.fold)\n",
    "        # return '../../../../feature_extractor_6class/VPC_embeddings/{}'.format(self.fold)\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['not_implemented']\n",
    "#         return [f for f in os.listdir(self.root + '/processed') if f.split('_')[-1] == '10.pt']\n",
    "\n",
    "    def download(self):\n",
    "        # Download to `self.raw_dir`.\n",
    "        print('download')\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        patch_size = 256 #512\n",
    "        raw_paths = os.listdir(self.raw_paths[0])\n",
    "        if '.ipynb_checkpoints' in raw_paths: raw_paths.remove('.ipynb_checkpoints')\n",
    "        for num, raw_path in enumerate(raw_paths):\n",
    "            print(f'{raw_path}, {num}/{len(raw_paths)}')\n",
    "            ## if VPC\n",
    "            # y = GG_from_core_index(self.df, raw_path) # For VPC\n",
    "            # y = Biomax_GG_from_core(self.df1, self.df3, raw_path) # for Biomax\n",
    "            # if y == -1: # label is unknown: 'S' or 'X' ... or '-' in Biomax\n",
    "            #     continue\n",
    "            ## elif Zurich\n",
    "            # y = int(self.df.GG[self.df.name == raw_path]) # This one line for Zurich\n",
    "            y = int(self.df.isup_grade[self.df.image_id == raw_path]) # This one line for PANDA\n",
    "            ## end if\n",
    "            embd_paths = []\n",
    "            for magnification in self.magnifications:\n",
    "            #     embd_paths.append(self.raw_paths[0] + '/' + raw_path + '/512/{}/256_aug_model_avgpool.pkl'.format(magnification))\n",
    "                embd_paths.append(self.raw_paths[0] + '/' + raw_path + '/{}/{}/256_aug_model_avgpool.pkl'.format(patch_size, magnification))\n",
    "            m = len(self.magnifications)\n",
    "            embd_dicts = []\n",
    "            for embd_path in embd_paths:\n",
    "                with open(embd_path, 'rb') as f:\n",
    "                    embd_dicts.append(pickle.load(f))\n",
    "            dict_keys = [k for k in embd_dicts[0].keys()]\n",
    "            dict_keys.sort(key=natural_keys)\n",
    "            if '.ipynb_checkpo' in dict_keys: dict_keys.remove('.ipynb_checkpo')\n",
    "            x = np.zeros((m, len(dict_keys), 512)) # 512 is hard coded! \n",
    "            edge = []\n",
    "            ### 2*len(dict_keys) for undirected graph times 2!\n",
    "            edge_type_size = {'10_10': 0, '20_20': 0, '40_40': 0, '10_20': 2*len(dict_keys), '20_40': 2*len(dict_keys)} # hard coded!\n",
    "            ## 10_10, 20_20, 40_40\n",
    "            for k, embd_dict in enumerate(embd_dicts):\n",
    "                for i, key in enumerate(dict_keys):\n",
    "                    x[k, i] = embd_dicts[k][key]\n",
    "                    for j, key_n in enumerate(dict_keys):\n",
    "                        if is_neighbors(key, key_n, patch_size): #  and i != j\n",
    "                            edge.append([i*m+k,j*m+k]) # [node0_mag10, node0_mag20, node0_mag40, node1_mag10, ...]\n",
    "                            if k == 0:\n",
    "                                edge_type_size['10_10'] += 1\n",
    "                            elif k == 1:\n",
    "                                edge_type_size['20_20'] += 1\n",
    "                            else:\n",
    "                                edge_type_size['40_40'] += 1\n",
    "            # 10_20\n",
    "            for i in range(len(dict_keys)):\n",
    "                edge.append([i*m,i*m+1])\n",
    "                edge.append([i*m+1,i*m])\n",
    "            # 20_40\n",
    "            for i in range(len(dict_keys)):\n",
    "                edge.append([i*m+1,i*m+2])\n",
    "                edge.append([i*m+2,i*m+1])\n",
    "            \n",
    "            # change x from 3d to 2d in the correct order\n",
    "            x = np.reshape(x, (x.shape[0]*x.shape[1], x.shape[2]), order='F')\n",
    "            # construct the edges\n",
    "            e = torch.tensor(edge, dtype=torch.long).t()\n",
    "            edge_index = e.detach()\n",
    "            ## mapping 10_10 to 0, ...\n",
    "            edge_type = [0]*edge_type_size['10_10']\n",
    "            edge_type.extend([1]*edge_type_size['20_20'])\n",
    "            edge_type.extend([2]*edge_type_size['40_40'])\n",
    "            edge_type.extend([3]*edge_type_size['10_20'])\n",
    "            edge_type.extend([4]*edge_type_size['20_40'])\n",
    "            \n",
    "            edge_type = torch.tensor(edge_type, dtype=torch.long).detach()\n",
    "            data = Data(x=torch.from_numpy(x), edge_index=edge_index, y=torch.tensor([y]))\n",
    "            if graph_type == 'heterogeneous':\n",
    "                data = data.to_heterogeneous(edge_type=edge_type)\n",
    "            # print(raw_path)\n",
    "            \n",
    "#             networkX_graph = to_networkx(data, node_attrs=[\"x\"]) # change the order of edges :((\n",
    "#             node_pos = [[int(k.split('_')[0]) + (i%3)*150, int(k.split('_')[1]) + (i%3)*150] for k in dict_keys for i in range(m)]\n",
    "#             nx.draw_networkx(networkX_graph, pos=node_pos)\n",
    "            \n",
    "#             G = nx.Graph()\n",
    "#             G.add_nodes_from(range(m*len(dict_keys)))\n",
    "#             G.add_edges_from([(f[0], f[1], {'color':edge_type[i]}) for i, f in enumerate(edge)])\n",
    "    \n",
    "#             plt.figure(1,figsize=(16,16))\n",
    "#             nx.draw_networkx(G, pos=node_pos, edge_color=[G[u][v]['color'] for u,v in G.edges()])\n",
    "#             plt.show()\n",
    "            \n",
    "            if self.pre_filter is not None and not self.pre_filter(data):\n",
    "                continue\n",
    "\n",
    "            if self.pre_transform is not None:\n",
    "                data = self.pre_transform(data)\n",
    "            \n",
    "            torch.save(data, os.path.join(self.processed_dir, f'{raw_path}_{graph_type}.pt'))\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.processed_file_names)\n",
    "\n",
    "    def get(self, idx):\n",
    "        data = torch.load(os.path.join(self.processed_dir, self.processed_file_names[idx]))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9a7091",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset = VPCDataset('Biomax_data/', fold, [10, 20, 40])\n",
    "# dataset = VPCDataset('VPC_Zurich_data/', fold, [10, 20, 40], path_outcomes)\n",
    "# dataset = VPCDataset('Zurich_data/', fold, [10, 20, 40], path_outcomes)\n",
    "dataset = VPCDataset('Radboud_data/', fold, [10, 20, 40], path_outcomes)\n",
    "# dataset = VPCDataset('data/', fold, [10, 20, 40], path_outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7af469",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24553846",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[[1,2],[3,4]],[[5,6],[7,8]], [[9,10],[11,12]]])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91abae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.reshape(a, (3*2,2), order='F'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fea206-560a-4fab-ad95-ac0b4b9a116e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
